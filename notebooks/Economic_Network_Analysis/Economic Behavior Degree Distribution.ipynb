{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from numpy import arange, log10\n",
    "\n",
    "# Import custom functions, add path ./../functions to sys.path\n",
    "import sys\n",
    "sys.path.append('./../../scripts/')\n",
    "from functions import chunk, agg, finalize\n",
    "\n",
    "# Set up Matplotlib to display plots inline\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/samplings/YMxpb_size2000.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/dataframe/io/csv.py:761\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\n\u001b[1;32m    749\u001b[0m     urlpath,\n\u001b[1;32m    750\u001b[0m     blocksize\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    760\u001b[0m ):\n\u001b[0;32m--> 761\u001b[0m     \u001b[39mreturn\u001b[39;00m read_pandas(\n\u001b[1;32m    762\u001b[0m         reader,\n\u001b[1;32m    763\u001b[0m         urlpath,\n\u001b[1;32m    764\u001b[0m         blocksize\u001b[39m=\u001b[39;49mblocksize,\n\u001b[1;32m    765\u001b[0m         lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m    766\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    767\u001b[0m         sample\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    768\u001b[0m         sample_rows\u001b[39m=\u001b[39;49msample_rows,\n\u001b[1;32m    769\u001b[0m         enforce\u001b[39m=\u001b[39;49menforce,\n\u001b[1;32m    770\u001b[0m         assume_missing\u001b[39m=\u001b[39;49massume_missing,\n\u001b[1;32m    771\u001b[0m         storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    772\u001b[0m         include_path_column\u001b[39m=\u001b[39;49minclude_path_column,\n\u001b[1;32m    773\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    774\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/dataframe/io/csv.py:561\u001b[0m, in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m b_lineterminator \u001b[39m=\u001b[39m lineterminator\u001b[39m.\u001b[39mencode()\n\u001b[0;32m--> 561\u001b[0m b_out \u001b[39m=\u001b[39m read_bytes(\n\u001b[1;32m    562\u001b[0m     urlpath,\n\u001b[1;32m    563\u001b[0m     delimiter\u001b[39m=\u001b[39;49mb_lineterminator,\n\u001b[1;32m    564\u001b[0m     blocksize\u001b[39m=\u001b[39;49mblocksize,\n\u001b[1;32m    565\u001b[0m     sample\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    566\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    567\u001b[0m     include_path\u001b[39m=\u001b[39;49minclude_path_column,\n\u001b[1;32m    568\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(storage_options \u001b[39mor\u001b[39;49;00m {}),\n\u001b[1;32m    569\u001b[0m )\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m include_path_column:\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/bytes/core.py:111\u001b[0m, in \u001b[0;36mread_bytes\u001b[0;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, include_path, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot do chunked reads on compressed files. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo read, set blocksize=None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[0;32m--> 111\u001b[0m size \u001b[39m=\u001b[39m fs\u001b[39m.\u001b[39;49minfo(path)[\u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/fsspec/implementations/local.py:87\u001b[0m, in \u001b[0;36mLocalFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strip_protocol(path)\n\u001b[0;32m---> 87\u001b[0m out \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path, follow_symlinks\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     88\u001b[0m link \u001b[39m=\u001b[39m stat\u001b[39m.\u001b[39mS_ISLNK(out\u001b[39m.\u001b[39mst_mode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/samplings/YMxpb_size2000.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/Economic Behavior Degree Distribution.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/Economic%20Behavior%20Degree%20Distribution.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_bins):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/Economic%20Behavior%20Degree%20Distribution.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdrive_path\u001b[39m}\u001b[39;00m\u001b[39m/samplings/YMxpb_size20\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(dataset_i)\u001b[39m.\u001b[39mzfill(\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/Economic%20Behavior%20Degree%20Distribution.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     df \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39;49mread_csv(filename, usecols\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mYEAR\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mID\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mVAT\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mVART_sum\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/Economic%20Behavior%20Degree%20Distribution.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     df_list\u001b[39m.\u001b[39mappend(df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/Economic%20Behavior%20Degree%20Distribution.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Concatenate all DataFrames and compute the sum of 'VART_sum'\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling the \u001b[39m\u001b[39m{\u001b[39;00mfuncname(func)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmethod registered to the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m}\u001b[39;00m\u001b[39m backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/samplings/YMxpb_size2000.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define custom aggregation functions using Dask\n",
    "tunique = dd.Aggregation('tunique', chunk, agg, finalize)\n",
    "first = dd.Aggregation('first', chunk, agg, finalize)\n",
    "\n",
    "# Define the path to the dataset\n",
    "drive_path = '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013'\n",
    "\n",
    "# Define the number of bins and initialize a list to hold DataFrames\n",
    "n_bins = 20\n",
    "df_list = []\n",
    "\n",
    "# Read data from CSV files and append to the list\n",
    "for dataset_i in range(n_bins):\n",
    "    filename = f'{drive_path}/samplings/YMxpb_size20{str(dataset_i).zfill(2)}.csv'\n",
    "    df = dd.read_csv(filename, usecols=['YEAR', 'ID', 'VAT', 'VART_sum'])\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames and compute the sum of 'VART_sum'\n",
    "data = dd.concat(df_list)\n",
    "data = data.groupby(['YEAR', 'ID', 'VAT'])['VART_sum'].sum().reset_index()\n",
    "\n",
    "# Compute and convert the Dask DataFrame to a Pandas DataFrame\n",
    "with ProgressBar():\n",
    "    out = data.compute()\n",
    "\n",
    "# Save the computed data to a CSV file\n",
    "out.to_csv('./../../data/buyer_seller_links.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1997\n",
      "[###############                         ] | 37% Completed | 22.9s"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from numpy import arange, log10\n",
    "\n",
    "\n",
    "for window in [1, 3, 5]:\n",
    "    gap = (window - 1) / 2\n",
    "    center_years = arange(1997, 2014, 2)\n",
    "\n",
    "    for Yc in center_years:\n",
    "        print(f'Window: {window}, Center Year: {Yc}')\n",
    "\n",
    "        # Filter data for the given window and center year\n",
    "        data_sec = data.loc[data.YEAR - Yc <= gap]\n",
    "\n",
    "        # Calculate ID_degree\n",
    "        data_sec_by_ID = data_sec.groupby(['ID']).agg({'VAT': tunique, 'VART_sum': sum})\n",
    "        ID_degree = data_sec_by_ID[['VAT']].reset_index()\n",
    "        ID_degree.columns = ['ID', 'ID_degree']\n",
    "        ID_degree['center_year'] = Yc\n",
    "        ID_degree['window'] = window\n",
    "\n",
    "        # Compute and save ID_deg\n",
    "        with ProgressBar():\n",
    "            ID_deg = ID_degree.compute()\n",
    "            ID_deg['bin'] = pd.cut(log10(ID_deg['ID_degree']), bins=arange(-.49, 5.99, .25))\n",
    "            ID_deg.to_csv('ID_deg_'+str(Yc)+'_'+str(window)+'.csv')\n",
    "\n",
    "        # Sample IDs\n",
    "        sampling = ID_deg.groupby(['bin'], observed=True).apply(lambda x: x.sample(200, replace=True))\n",
    "        data_sec_sample = data_sec.loc[data_sec.ID.isin(sampling['ID'].values)]\n",
    "\n",
    "        # Calculate VAT_degree\n",
    "        data_sec_by_VAT = data_sec_sample.groupby(['VAT']).agg({'ID': tunique, 'VART_sum': sum})\n",
    "        VAT_degree = data_sec_by_VAT[['ID']].reset_index()\n",
    "        VAT_degree.columns = ['VAT', 'VAT_degree']\n",
    "        VAT_degree['center_year'] = Yc\n",
    "        VAT_degree['window'] = window\n",
    "\n",
    "        # Compute and save VAT_deg\n",
    "        with ProgressBar():\n",
    "            VAT_deg = VAT_degree.compute()\n",
    "            VAT_deg.to_csv('VAT_deg_save_'+str(Yc)+'_'+str(window)+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from numpy import arange, log10, power\n",
    "\n",
    "# Set matplotlib to inline and define a function for histogram plotting\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_histogram(data, column, bins_range):\n",
    "    plt.hist(log10(data[column]), bins=bins_range)\n",
    "    plt.show()\n",
    "\n",
    "# Define path for data\n",
    "data_path = './../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate and compute VAT degrees\n",
    "VAT_degree_res = dd.concat(VAT_degree_res)  # Assuming VAT_degree_res is defined elsewhere\n",
    "with ProgressBar():\n",
    "    VAT_deg = VAT_degree_res.compute()\n",
    "\n",
    "# Save VAT degrees to file\n",
    "VAT_deg.to_csv(data_path + 'VAT_deg_save.csv', index=False)\n",
    "\n",
    "# ID_deg_res = dd.concat(ID_degree_res)\n",
    "# with ProgressBar():\n",
    "#     ID_deg = ID_deg_res.compute()\n",
    "# ID_deg.to_csv(data_path + 'ID_deg_save.csv', index=False)\n",
    "\n",
    "# Merge and compute assortativity information\n",
    "# Assuming data_sec_sample, ID_degree, VAT_degree are defined and loaded\n",
    "assortativity_info = data_sec_sample.groupby(['ID', 'VAT'])[['VART_sum']].mean().reset_index().merge(\n",
    "    ID_degree).merge(VAT_degree)\n",
    "\n",
    "with ProgressBar():\n",
    "    assortativity_df = assortativity_info.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histogram of ID degrees\n",
    "plot_histogram(ID_deg, 'ID_degree', arange(-.49, 5.99, .25))  # Assuming ID_deg is defined and loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating bins for degree data\n",
    "left = power(10, arange(-.1, 5, .2)).round(1)\n",
    "right = power(10, arange(-.1, 5, .2) + .2).round(1)\n",
    "bins = pd.IntervalIndex.from_arrays(left, right)\n",
    "\n",
    "# Sample IDs and prepare data for assortativity analysis\n",
    "# Assuming ID_deg_part is a filtered version of ID_deg\n",
    "ID_deg_part = ID_deg.loc[(ID_deg.center_year == Yc) & (ID_deg.window == window)]\n",
    "ID_deg_part.groupby(pd.cut(ID_deg_part['ID_degree'], bins), observed = True).count().sort_index()\n",
    "\n",
    "sample_IDs = ID_deg_part.groupby(pd.cut(ID_deg_part['ID_degree'], bins), observed=True).apply(\n",
    "    lambda x: x.sample(1000, replace=True))['ID'].values\n",
    "data_sec = data.loc[(data.YEAR - Yc <= gap) & (data.ID.isin(sample_IDs))]  # Assuming data, Yc, gap are defined\n",
    "\n",
    "# Compute summary results\n",
    "summary_results = []\n",
    "for df_degrees in results:  # Assuming results is defined\n",
    "    with ProgressBar():\n",
    "        x = df_degrees.compute()\n",
    "    summary_result = x.groupby(pd.cut(x['ID_degree'], bins)).agg({...})  # Add relevant aggregation\n",
    "    summary_results.append(summary_result)\n",
    "\n",
    "# Concatenate and save summary results\n",
    "pd.concat(summary_results).dropna().to_csv(data_path + 'assortativity_summary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting Quantile Plots\n",
    "# Assuming df_degrees is defined and loaded\n",
    "fig, ax = plt.subplots(1)\n",
    "df_degrees.groupby('VAT_degree_bin').quantile([.25, .5, .75]).plot(\n",
    "    x='VAT_degree', y='ID_degree', marker='', ax=ax)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Choose a bin_size, \n",
    "# size_df_list = n_size_bins * [ None ]\n",
    "\n",
    "# for s in range(14, n_size_bins):\n",
    "\n",
    "#     bs_ix_df = exp_index.loc[exp_index.size_bins == s]\n",
    "\n",
    "#     size_i_df_list = []\n",
    "#     for dataset_i in bs_ix_df.exp_mma_cat.unique():\n",
    "#         df = df_list[dataset_i]\n",
    "#         size_i_df_list += [df.loc[df.ID.isin(bs_ix_df.ID)]]\n",
    "\n",
    "#         size_df_list[s] = pd.concat(size_i_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
