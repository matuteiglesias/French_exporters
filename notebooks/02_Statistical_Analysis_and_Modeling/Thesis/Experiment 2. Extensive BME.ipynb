{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../../data/processed/ID_Y.csv')\n",
    "# df = pd.read_csv('./../../data/processed/.csv')\n",
    "\n",
    "sales = df.loc[df.IMPORT == 1].groupby(['ID', 'YEAR'])['VART'].sum().unstack()\n",
    "sales = sales.loc[sales.sum(1).sort_values().index]\n",
    "\n",
    "logsales = np.log10(sales)\n",
    "demlogsales = logsales.subtract(logsales.mean(1), axis = 0)\n",
    "\n",
    "sizes = sales.sum(1)\n",
    "\n",
    "\n",
    "Q = 10\n",
    "parts = pd.cut(sizes.cumsum()/sizes.sum(), Q, labels = range(Q)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.172793795363236e+21\n",
      "8.172793795363238e+21\n",
      "8.939063483056481e+21\n",
      "9.696943094270655e+21\n"
     ]
    }
   ],
   "source": [
    "Sk = sales.groupby(parts).sum()\n",
    "# yqs.T.cov().sum().sum()\n",
    "\n",
    "X = Sk.sum() # == sales.sum()\n",
    "EX = X.mean()\n",
    "\n",
    "# Exact\n",
    "print(Sk.T.cov().sum().sum())\n",
    "print(X.var())\n",
    "\n",
    "# Aggregate approx\n",
    "print(np.log10(X).var()*(np.log(10)*EX)**2)\n",
    "\n",
    "# Sectoral log fluctuations\n",
    "# Sq = yqs.mean(1).mean() # approx un sq que es el valor lineal medio de los qs\n",
    "# Sq**2*np.log(10)**2*(np.log10(yqs).T.cov().sum().sum())\n",
    "print((EX/Q)**2*np.log(10)**2*(np.log10(Sk).T.cov().sum().sum()))\n",
    "\n",
    "logsales = np.log10(sales)\n",
    "\n",
    "micro_s = logsales.sub(logsales.mean(1), axis=0).unstack().std()\n",
    "zero_shock = logsales.notna().multiply(logsales.mean(1), axis = 0).replace(0, np.nan)\n",
    "\n",
    "# noise = np.power(10, zero_shock + simu_shocks) - np.power(10, zero_shock)\n",
    "# common_R + noise_qs + base_qs - Sk == 0\n",
    "\n",
    "nqs = parts.value_counts().values\n",
    "lognqs = np.log10(nqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The outcome of bootstrap is having the zero shock time series of quantiles and the 'all included time series of quantiles'.\n",
    "\n",
    "n = 100\n",
    "Q = 10\n",
    "\n",
    "\n",
    "df = pd.read_csv('./../../data/processed/ID_Y.csv')\n",
    "# df = pd.read_csv('./../../data/processed/.csv')\n",
    "\n",
    "\n",
    "for i in [0, 1]:\n",
    "    sales = df.loc[df.IMPORT == i].groupby(['ID', 'YEAR'])['VART'].sum().unstack()\n",
    "    sales = sales.loc[sales.sum(1).sort_values().index]\n",
    "\n",
    "#     sample = sales.sample(frac = .5); sample = sample.loc[sample.sum(1).sort_values().index] #sorting\n",
    "#     sizes = sample.loc[sample.sum(1).sort_values().index].sum(1)\n",
    "#     parts = pd.cut(sizes.cumsum()/sizes.sum(), Q, labels = range(Q)).sort_index()\n",
    "#     eff_nq = sample.groupby(parts).count().mean(1).round().astype(int)\n",
    "\n",
    "    base_out_list = []\n",
    "    total_out_list = []\n",
    "\n",
    "\n",
    "    for m in range(n):\n",
    "        if m%10 == 0: print(m)\n",
    "\n",
    "        df_bs_ = sales.sample(frac = .5)\n",
    "\n",
    "        for size_sorting in [True, False]:\n",
    "\n",
    "            for s in [.02, .05, .1, .25, .5]:\n",
    "\n",
    "                if size_sorting: \n",
    "                    df_bs = df_bs_.loc[df_bs_.sum(1).sort_values().index] #sorting\n",
    "                else:\n",
    "                    df_bs = df_bs_\n",
    "\n",
    "                logsales = np.log10(df_bs)\n",
    "\n",
    "                micro_s = logsales.sub(logsales.mean(1), axis=0).unstack().std()\n",
    "                zero_shock = logsales.notna().multiply(logsales.mean(1), axis = 0).replace(0, np.nan)\n",
    "\n",
    "                simu_shocks = (s/micro_s)*logsales.sub(logsales.mean(1), axis = 0)\n",
    "\n",
    "                total = np.power(10, zero_shock + simu_shocks)\n",
    "                base = np.power(10, zero_shock)\n",
    "\n",
    "                total['q'] = pd.cut(total.sum(1).cumsum(), Q, labels = range(Q))\n",
    "                nqs = total['q'].value_counts().values\n",
    "\n",
    "\n",
    "\n",
    "                base_yqs = base.groupby(total['q']).sum()\n",
    "                total_yqs = total.groupby(total['q']).sum()        \n",
    "\n",
    "                base_yqs['m'] = m; base_yqs['nqs'] = nqs; base_yqs['s'] = s; base_yqs['parts'] = ['P', 'Q'][size_sorting];\n",
    "                base_out_list += [base_yqs]\n",
    "\n",
    "                total_yqs['m'] = m; total_yqs['nqs'] = nqs; total_yqs['s'] = s; total_yqs['parts'] = ['P', 'Q'][size_sorting];\n",
    "                total_out_list += [total_yqs]\n",
    "\n",
    "    bs_base = pd.concat(base_out_list).reset_index().set_index(['q', 'm', 's', 'nqs'])\n",
    "    bs_totl = pd.concat(total_out_list).reset_index().set_index(['q', 'm', 's', 'nqs'])\n",
    "\n",
    "    # SAVE DATA\n",
    "    bs_base.to_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv')\n",
    "    bs_totl.to_csv('./../../data/bootstraps/bs_totl_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "#### LOAD DATA\n",
    "\n",
    "n = 100\n",
    "Q = 10\n",
    "\n",
    "# legacy\n",
    "# size_sorting = False\n",
    "# bs_base = pd.read_csv('./../../data/bootstraps/bs_base_'+['random', 'sorted'][size_sorting]+'_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "# bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_'+['random', 'sorted'][size_sorting]+'_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "\n",
    "bs_base = pd.read_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "\n",
    "bs_base = bs_base.loc[bs_base.parts == 'P'].drop('parts', axis = 1)\n",
    "bs_totl = bs_totl.loc[bs_totl.parts == 'P'].drop('parts', axis = 1)\n",
    "\n",
    "### ADDITIVE DECOMPOSITION\n",
    "\n",
    "bs_base = bs_base.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "bs_totl = bs_totl.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "fe_n = (bs_totl - bs_base)\n",
    "fe = fe_n.groupby(level = ['s', 'q']).transform('median') \n",
    "noise = fe_n - fe\n",
    "\n",
    "### MULTIPLICATIVE DECOMPOSITION\n",
    "\n",
    "mul_fe_noise = bs_totl/bs_base\n",
    "# Now, I use the bs to estimate disentangling fe from noise\n",
    "mul_fe = mul_fe_noise.groupby(level = ['s', 'q']).transform('median') \n",
    "mul_noise = mul_fe_noise/mul_fe\n",
    "\n",
    "# Important: they fulfill\n",
    "# bs_totl - bs_base*mul_fe*mul_noise == 0\n",
    "# np.log10(bs_totl) - np.log10(bs_base) - np.log10(mul_fe) - np.log10(mul_noise) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "Q = 10\n",
    "\n",
    "# non-anonymous function\n",
    "def pct10 (x): \n",
    "    return np.percentile(x, q=10)\n",
    "\n",
    "def pct25 (x): \n",
    "    return np.percentile(x, q=25)\n",
    "\n",
    "def pct75 (x): \n",
    "    return np.percentile(x, q=75)\n",
    "\n",
    "def pct90 (x): \n",
    "    return np.percentile(x, q=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### LOAD DATA\n",
    "\n",
    "for i in [0, 1]:\n",
    "    for size_sorting in [True, False]:\n",
    "    #     bs_base = pd.read_csv('./../../data/bootstraps/bs_base_'+['random', 'sorted'][size_sorting]+'_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "    #     bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_'+['random', 'sorted'][size_sorting]+'_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "\n",
    "        bs_base = pd.read_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "        bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "        bs_base = bs_base.loc[bs_base.parts == ['P', 'Q'][size_sorting]].drop('parts', axis = 1)\n",
    "        bs_totl = bs_totl.loc[bs_totl.parts == ['P', 'Q'][size_sorting]].drop('parts', axis = 1)\n",
    "\n",
    "        ### ADDITIVE DECOMPOSITION\n",
    "\n",
    "        bs_base = bs_base.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "        bs_totl = bs_totl.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "        fe_n = (bs_totl - bs_base)\n",
    "        fe = fe_n.groupby(level = ['s', 'q']).transform('median') \n",
    "        noise = fe_n - fe\n",
    "\n",
    "        ### MULTIPLICATIVE DECOMPOSITION\n",
    "\n",
    "        mul_fe_noise = bs_totl/bs_base\n",
    "        # Now, I use the bs to estimate disentangling fe from noise\n",
    "        mul_fe = mul_fe_noise.groupby(level = ['s', 'q']).transform('median') \n",
    "        mul_noise = mul_fe_noise/mul_fe\n",
    "\n",
    "        # Important: they fulfill\n",
    "        # bs_totl - bs_base*mul_fe*mul_noise == 0\n",
    "        # np.log10(bs_totl) - np.log10(bs_base) - np.log10(mul_fe) - np.log10(mul_noise) == 0\n",
    "\n",
    "        for linear in [True, False]:\n",
    "\n",
    "            if linear:\n",
    "                BASE = bs_base\n",
    "                FE = fe\n",
    "                NOISE = noise\n",
    "            else:\n",
    "                BASE = np.log10(bs_base) # bs_base\n",
    "                FE = np.log10(mul_fe) # fe\n",
    "                NOISE = np.log10(mul_noise)# noise\n",
    "\n",
    "\n",
    "            result_list = []\n",
    "            obs_parts_cross_cov_list = []\n",
    "\n",
    "            for m in range(n):\n",
    "                if m%50 == 0: print(m)\n",
    "                for s in [.02, .05, .1, .25, .5]:\n",
    "                    base_m = BASE.loc[(BASE.index.get_level_values('m') == m) & (BASE.index.get_level_values('s') == s)]\n",
    "                    fe_m = FE.loc[(FE.index.get_level_values('m') == m) & (FE.index.get_level_values('s') == s)]\n",
    "                    n_m = NOISE.loc[(NOISE.index.get_level_values('m') == m) & (NOISE.index.get_level_values('s') == s)]\n",
    "\n",
    "                    obs_s_m = pd.concat([base_m, fe_m, n_m])\n",
    "                    obs_s_m.index = pd.MultiIndex.from_product([['base', 'FE', 'noise'],range(Q)], names = ['comp', 'part'])\n",
    "\n",
    "                    cov = obs_s_m.T.cov() # shape: 3Q x 3Q\n",
    "\n",
    "                    ####### OUTCOME 1\n",
    "        #           Save observed cross covariance of the parts\n",
    "\n",
    "    #                 obs_parts_cross_cov = cov.loc['noise'][['noise']].stack(); obs_parts_cross_cov.index.names = ['partA', 'partB']; obs_parts_cross_cov.columns = ['cov']\n",
    "    #                 obs_parts_cross_cov['m'] = m; obs_parts_cross_cov['s'] = s\n",
    "    #                 obs_parts_cross_cov = obs_parts_cross_cov.reset_index().set_index(['s', 'm', 'partA', 'partB'])\n",
    "\n",
    "    #                 obs_parts_cross_cov_list += [obs_parts_cross_cov]\n",
    "\n",
    "                    obs_parts_cross_cov = pd.DataFrame(cov.stack([0, 1]), columns = ['cov'])\n",
    "                    obs_parts_cross_cov.index.names = ['compA', 'partA', 'compB', 'partB']\n",
    "                    obs_parts_cross_cov['m'] = m; obs_parts_cross_cov['s'] = s\n",
    "                    obs_parts_cross_cov = obs_parts_cross_cov.reset_index().set_index(['s', 'm', 'compA', 'partA', 'compB', 'partB'])\n",
    "\n",
    "                    obs_parts_cross_cov_list += [obs_parts_cross_cov]\n",
    "\n",
    "\n",
    "                    # np.log10(abs(obs_s_m.T.cov())).round(1)\n",
    "\n",
    "                    ####### OUTCOME 2\n",
    "                    # Characterize diagonal and offdiagonal for all 3x3 cross components\n",
    "\n",
    "                    ## Make a mask for i = j\n",
    "                    ## Groupby the other level and (we need to sum) but possibly describe.\n",
    "                    part_index = np.array(3*Q*[cov.index.get_level_values('part').values])\n",
    "\n",
    "                    ## Mask for off diagonals\n",
    "                    part_diag_mask = np.equal(part_index, part_index.T)\n",
    "                    pd_diag_mask = pd.DataFrame(part_diag_mask, index = cov.index, columns = cov.columns)\n",
    "\n",
    "\n",
    "                    diag = cov[pd_diag_mask].stack([-2, -1]).groupby(level = [0, 2]).agg(['count', np.mean, np.sum, np.std, \n",
    "                                                                                          pct10, pct25, pct75, pct90])\n",
    "                    diag = pd.concat([diag], keys=['diag'], names=['diag'], axis = 0)\n",
    "\n",
    "                    offdiag = cov[~pd_diag_mask].stack([-2, -1]).groupby(level = [0, 2]).agg(['count', np.mean, np.sum, np.std,\n",
    "                                                                                          pct10, pct25, pct75, pct90])\n",
    "                    offdiag = pd.concat([offdiag], keys=['offdiag'], names=['diag'], axis = 0)\n",
    "\n",
    "\n",
    "                    result_m = pd.concat([diag, offdiag])\n",
    "                    result_m = pd.concat([result_m], keys=[m], names=['m'], axis = 0)\n",
    "                    result_m = pd.concat([result_m], keys=[s], names=['s'], axis = 0)\n",
    "                    result_m.index.names = ['s', 'm', 'diag', 'compA', 'compB']\n",
    "\n",
    "                    result_list += [result_m]\n",
    "\n",
    "            result_varname = '_'.join(['cov_elements_desc', ['random', 'sorted'][size_sorting], ['log', 'lin'][linear], ['X', 'M'][i]]); print(result_varname)\n",
    "            result = pd.concat(result_list)\n",
    "            result.to_csv('./../../data/bootstraps/'+result_varname+'.csv')\n",
    "            globals()[result_varname] = result\n",
    "\n",
    "\n",
    "            result_varname = '_'.join(['parts_cross_cov', ['random', 'sorted'][size_sorting], ['log', 'lin'][linear], ['X', 'M'][i]]); print(result_varname)\n",
    "            result = pd.concat(obs_parts_cross_cov_list)\n",
    "            result.to_csv('./../../data/bootstraps/'+result_varname+'.csv')            \n",
    "            globals()[result_varname] = result\n",
    "\n",
    "    #         if linear:\n",
    "    #             result_lin = pd.concat(result_list)\n",
    "    #             result_parts_cross_cov_lin = pd.concat(obs_parts_cross_cov_list) ## adapt\n",
    "    #         else:\n",
    "    #             result_log = pd.concat(result_list)\n",
    "    #             result_parts_cross_cov_log = pd.concat(obs_parts_cross_cov_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameError: name 'result_log' is not defined\n",
    "result_log_scaled = result_log.copy()\n",
    "result_log_scaled[['mean','sum','std']] = component_scale_factor*result_log[['mean','sum','std']]\n",
    "\n",
    "component_scale_factor = (((Sk/2).mean(1)*np.log(10))**2).mean()   ## This is (Sq*ln10)**2, but Sq/2 because of BS\n",
    "component_scale_factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
