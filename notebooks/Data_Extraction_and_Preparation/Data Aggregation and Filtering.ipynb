{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data reading and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def aggregation(chunk, index, func):\n",
    "    grouped_object = chunk.groupby(index,sort = False)\n",
    "    # not sorting results in a minor speedup\n",
    "    return grouped_object.agg(func)\n",
    "\n",
    "def merge_ref(df, filter_ref, filter_ix):\n",
    "    return df.reset_index().merge(filter_ref, on = filter_ix, how = 'left').set_index('index')\n",
    "     \n",
    "def select_entries(df, filter_col, value, by_df = None):\n",
    "    if by_df is None:\n",
    "        by_df = df\n",
    "        \n",
    "    if isinstance(value, (list,)):\n",
    "        return df.loc[by_df[filter_col].isin(value)]\n",
    "    else:\n",
    "        return df.loc[by_df[filter_col] == value]\n",
    "\n",
    "def aggregate_chunk(chunk, output_index, chunk_agg_func, verbose):\n",
    "    result_chunk = aggregation(chunk, output_index, chunk_agg_func) if aggregate_chunks else chunk\n",
    "    if verbose:\n",
    "        print(\"Number of rows \", result_chunk.shape[0])\n",
    "    return result_chunk\n",
    "\n",
    "def process_year(year, n_chunks, data_path, columns, output_index, chunk_agg_func, verbose):\n",
    "    size = np.ceil(nrows1[year] / n_chunks)\n",
    "    reader = pd.read_csv(f'{data_path}/DP1610_MAASTRICHT1_{year}.txt', chunksize=size, \n",
    "                         usecols=map(colname_no.get, columns), delimiter=';', header=None)\n",
    "    yr_result_list = []\n",
    "\n",
    "    for i in range(int(n_chunks)):\n",
    "        start_time = time.time()\n",
    "        chunk = next(reader).rename(colname_no, axis=1)\n",
    "        chunk['IMPORT'] = chunk['FLUX'] % 2\n",
    "\n",
    "        chunk = select_entries(chunk, filter_col='IMPORT', value=0)\n",
    "        chunk_ = merge_ref(chunk, exp_size_ref, filter_ix='ID')\n",
    "        chunk = select_entries(chunk, filter_col='exp_mma_cat', value=bin_, by_df=chunk_)\n",
    "\n",
    "        yr_result_list.append(aggregate_chunk(chunk, output_index, chunk_agg_func, verbose))\n",
    "        if verbose:\n",
    "            print(\"Loop \", i, \"took %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "    return pd.concat(yr_result_list)\n",
    "\n",
    "def save_yearly_aggregated_result(year_result, output_index, yr_agg_func, save_path, sampling_name, year):\n",
    "    yr_agg_result = aggregation(year_result, output_index, yr_agg_func)\n",
    "    yr_agg_result.index = pd.MultiIndex.from_tuples(list(yr_agg_result.index), names=tuple(output_index))\n",
    "    yr_agg_result.columns = yr_agg_result.columns.droplevel(1)\n",
    "    yr_agg_result.to_csv(f'{save_path}/{sampling_name}_{year}.csv')\n",
    "\n",
    "def main():\n",
    "    years = list(reversed(range(1997, 2014)))\n",
    "    n_chunks = 5\n",
    "    data_path = './../../data/type1/DP1610_MAASTRICHT1_1997_2013'\n",
    "    columns = ['YEAR', 'MONTH', 'FLUX', 'ID', 'CN ID 8', 'VAT', 'VART']\n",
    "    output_index = ['YEAR', 'MONTH', 'IMPORT', 'ID', 'CN ID 8', 'VAT']\n",
    "    chunk_agg_func = {'VART': ['sum']}\n",
    "    yr_agg_func = {'VART_sum': ['sum']}\n",
    "    verbose = True\n",
    "    save_path = './../../data/samplings'\n",
    "\n",
    "    for bin_ in reversed(range(5)):\n",
    "        sampling_name = f'YMxpb_size{str(n_bins).zfill(2)}{str(bin_).zfill(2)}'\n",
    "        all_year_results = []\n",
    "\n",
    "        for year in years:\n",
    "            print(f'Processing year: {year}')\n",
    "            year_result = process_year(year, n_chunks, data_path, columns, output_index, chunk_agg_func, verbose)\n",
    "            all_year_results.append(year_result)\n",
    "            save_yearly_aggregated_result(year_result, output_index, yr_agg_func, save_path, sampling_name, year)\n",
    "\n",
    "        pd.concat(all_year_results).to_csv(f'{save_path}/{sampling_name}.csv', index=True)\n",
    "        print(f'Saved at: {save_path}/{sampling_name}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions for chunk filtering and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/media/matias/Elements/export_france/data/'\n",
    "path1 = data_path+'type1/DP1610_MAASTRICHT1_1997_2013'\n",
    "path2 = data_path+'type2/DP1611_MAASTRICHT2_1997_2013'\n",
    "\n",
    "# Writing as a function\n",
    "\n",
    "                                \n",
    "colnames = [u'YEAR', u'MONTH', u'FLUX', u'ID', u'DEPT', u'CN ID 8', u'CPA6',\n",
    "       u'PYOD', u'PAYP', u'VAT', u'PRIFAC', u'DEVFAC', u'VFTE', u'VART', u'D_MASSE', u'MASSE', u'USUP', u'USUP_MT']\n",
    "\n",
    "colname_no = dict(zip(colnames, range(18)))\n",
    "colno_name = dict(zip(range(18), colnames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #We need to know number of rows of each file:\n",
    "# l = []\n",
    "# with open(data_path+'/type1_row_ct') as f:\n",
    "#     for line in f:\n",
    "#         l += [int(line.strip())]\n",
    "# nrows1 = dict(zip(range(1997, 2014), l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 1997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 1997 has 19303127 rows\n",
      "Processing year: 1998\n",
      "Year 1998 has 20906709 rows\n",
      "Processing year: 1999\n",
      "Year 1999 has 22004431 rows\n",
      "Processing year: 2000\n",
      "Year 2000 has 23044998 rows\n",
      "Processing year: 2001\n",
      "Year 2001 has 23172197 rows\n",
      "Processing year: 2002\n",
      "Year 2002 has 23696908 rows\n",
      "Processing year: 2003\n",
      "Year 2003 has 24498599 rows\n",
      "Processing year: 2004\n",
      "Year 2004 has 26093520 rows\n",
      "Processing year: 2005\n",
      "Year 2005 has 27798440 rows\n",
      "Processing year: 2006\n",
      "Year 2006 has 28949804 rows\n",
      "Processing year: 2007\n",
      "Year 2007 has 30802487 rows\n",
      "Processing year: 2008\n",
      "Year 2008 has 31545582 rows\n",
      "Processing year: 2009\n",
      "Year 2009 has 31393346 rows\n",
      "Processing year: 2010\n",
      "Year 2010 has 34968248 rows\n",
      "Processing year: 2011\n",
      "Year 2011 has 35979867 rows\n",
      "Processing year: 2012\n",
      "Year 2012 has 38311600 rows\n",
      "Processing year: 2013\n",
      "Year 2013 has 40902059 rows\n",
      "{1997: 19303127, 1998: 20906709, 1999: 22004431, 2000: 23044998, 2001: 23172197, 2002: 23696908, 2003: 24498599, 2004: 26093520, 2005: 27798440, 2006: 28949804, 2007: 30802487, 2008: 31545582, 2009: 31393346, 2010: 34968248, 2011: 35979867, 2012: 38311600, 2013: 40902059}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "nrows1 = {}\n",
    "years = range(1997, 2014)\n",
    "\n",
    "for y in years:\n",
    "    print(f'Processing year: {y}')\n",
    "    file_path = f'{path1}/DP1610_MAASTRICHT1_{y}.txt'\n",
    "\n",
    "    # Use wc -l to count the number of lines in the file\n",
    "    process = subprocess.Popen(['wc', '-l', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, err = process.communicate()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f'Error processing file for year {y}: {err}')\n",
    "    else:\n",
    "        # The output is 'number_of_lines filename', so split and take the first part\n",
    "        row_count = int(out.decode('utf-8').split()[0])\n",
    "        nrows1[y] = row_count\n",
    "        print(f'Year {y} has {row_count} rows')\n",
    "\n",
    "# nrows1 now contains the number of rows for each year\n",
    "print(nrows1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process references for filtering (eg. data on firm sizes to filter by size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming median_sizes is already computed and saved as per the previous script\n",
    "median_sizes = pd.read_csv('./../../data/processed/sizes_index.csv')\n",
    "\n",
    "# The median_sizes DataFrame already contains the 'exp_mma_cat' column, so we can directly use it\n",
    "# Create a reference dataframe that contains only the IDs and their corresponding 'exp_mma' category\n",
    "exp_size_ref = median_sizes[['ID', 'exp_mma_cat']]\n",
    "\n",
    "# Define column names for filtering operations\n",
    "filter_ix = 'ID'\n",
    "filter_col = 'exp_mma_cat'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop for reading from hard drive and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_bins' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Data Aggregation and Filtering.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Data%20Aggregation%20and%20Filtering.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32m/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Data Aggregation and Filtering.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Data%20Aggregation%20and%20Filtering.ipynb#X24sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m save_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./../../data/samplings\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Data%20Aggregation%20and%20Filtering.ipynb#X24sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m bin_ \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Data%20Aggregation%20and%20Filtering.ipynb#X24sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     sampling_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mYMxpb_size\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(n_bins)\u001b[39m.\u001b[39mzfill(\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(bin_)\u001b[39m.\u001b[39mzfill(\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Data%20Aggregation%20and%20Filtering.ipynb#X24sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     all_year_results \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Data%20Aggregation%20and%20Filtering.ipynb#X24sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mfor\u001b[39;00m year \u001b[39min\u001b[39;00m years:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_bins' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # names = tuple(index)\n",
    "# # sampling_name = name\n",
    "\n",
    "# years = list(reversed(range(1997, 2014)))\n",
    "# n_chunks = 5\n",
    "# data_path = data_path+'type1/DP1610_MAASTRICHT1_1997_2013'\n",
    "# columns = ['YEAR', 'MONTH', 'FLUX', 'ID', 'CN ID 8', 'VAT','VART']\n",
    "# aggregate_chunks = True\n",
    "# chunk_agg_func = {'VART':['sum']}\n",
    "# aggregate_years = True\n",
    "# yr_agg_func = {'VART_sum':['sum']}\n",
    "# verbose = True\n",
    "# output_index = ['YEAR', 'MONTH', 'IMPORT', 'ID', 'CN ID 8', 'VAT']\n",
    "\n",
    "# save_yr_agg_result = True\n",
    "# save_path = path1+'/samplings'\n",
    "\n",
    "# for bin_ in list(reversed(range(5))): #n_bins\n",
    "#     print 'bin no. '+str(bin_)\n",
    "#     sampling_name = 'YMxpb_size'+str(n_bins).zfill(2)+str(bin_).zfill(2)\n",
    "#     l = []\n",
    "\n",
    "#     for y in years:\n",
    "#         print 'year: '+str(y)\n",
    "#         #Rows for each chunk\n",
    "#         size = np.ceil(nrows1[y]/n_chunks)\n",
    "#         print 'max_chunk_size: '+str(size)\n",
    "\n",
    "#         reader = pd.read_csv(data_path+'/DP1610_MAASTRICHT1_'+str(y)+'.txt', chunksize = size, \n",
    "#                              usecols = map(colname_no.get, columns)\n",
    "#                              , delimiter = ';', header = None) #'CN ID 8', 'PYOD'\n",
    "\n",
    "#         yr_result_list = []\n",
    "\n",
    "#         #Loop over chunks\n",
    "#         for i in range(int(n_chunks)):\n",
    "#             start_time = time.time()\n",
    "#             chunk = next(reader).rename(colno_name, axis = 1)\n",
    "#             chunk['IMPORT'] = chunk['FLUX'] % 2 # so that '1' indicates imports\n",
    "\n",
    "#             \"\"\"\n",
    "#             Chunk filtering\n",
    "#             \"\"\"\n",
    "#             #select imports/exports\n",
    "#             chunk = select_entries(chunk, filter_col = 'IMPORT', value = 0)\n",
    "\n",
    "#             # select by firm size\n",
    "#             chunk_ = merge_ref(chunk, exp_size_ref, filter_ix = 'ID')\n",
    "#             chunk = select_entries(chunk, filter_col = 'exp_mma_cat', value = bin_, by_df = chunk_)\n",
    "\n",
    "#             \"\"\"\n",
    "#             Chunk aggregation\n",
    "#             \"\"\"\n",
    "#             if aggregate_chunks: \n",
    "#                 result_chunk = aggregation(chunk, output_index, chunk_agg_func)\n",
    "#             else: \n",
    "#                 result_chunk = chunk\n",
    "\n",
    "#     #             print result_chunk.head()\n",
    "\n",
    "#             yr_result_list += [result_chunk]\n",
    "#             if verbose: print(\"Number of rows \",result_chunk.shape[0])\n",
    "#             if verbose: print(\"Loop \",i,\"took %s seconds\" % (time.time() - start_time))\n",
    "#             del(result_chunk) \n",
    "\n",
    "\n",
    "#         concat_result = pd.concat(yr_result_list)\n",
    "#     #         print concat_result.head()\n",
    "\n",
    "#         # Unique users vs Number of rows after the first computation    \n",
    "#         if verbose: print(\"size of concat_result:\", len(concat_result))\n",
    "#     #         if verbose: print(\"unique firms in concat_result:\", len(concat_result.index.unique()))\n",
    "\n",
    "#         result = concat_result\n",
    "#         result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "\n",
    "#         if aggregate_years:\n",
    "#     #             yr_agg_result = result.groupby(index).agg(yr_agg_func)\n",
    "#             yr_agg_result = aggregation(result, output_index, yr_agg_func)\n",
    "#             yr_agg_result.index = pd.MultiIndex.from_tuples(list(yr_agg_result.index), names=tuple(output_index))\n",
    "#             yr_agg_result.columns = yr_agg_result.columns.droplevel(1)\n",
    "\n",
    "#         else:\n",
    "#             yr_agg_result = result\n",
    "\n",
    "#         if verbose: print(\"size of yr_agg_result:\", len(yr_agg_result))\n",
    "\n",
    "#         l += [yr_agg_result]\n",
    "\n",
    "#         if save_yr_agg_result:\n",
    "#             yr_agg_result.to_csv(save_path+'/'+sampling_name+'_'+str(y)+'.csv')\n",
    "\n",
    "#     filename = save_path+'/'+sampling_name+'.csv'\n",
    "#     pd.concat(l).to_csv(filename, index = True)\n",
    "#     print 'saved at: '+str(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Imports and exports by firm/year\n",
    "\n",
    "# IDs = firm_stats['ID']\n",
    "# Mns = [1, 2, 3]\n",
    "\n",
    "# def sample_data(columns, index = ['ID', 'IMPORT','YEAR'], chunk_agg = True, chunk_agg_func = {}, \n",
    "#             yr_agg_func = {}, name = 'test_sampling', n_chunks = 30.,\n",
    "#                 firm_filtering = True, firm_filtering_cols = ['ID'], firm_filtering_arrays = [IDs],\n",
    "#                 time_filtering = False, time_filtering_arrays = [Mns],\n",
    "#                 verbose = False, yr_chunks_agg = True, save_yr_agg_result = False,\n",
    "#                data_path = path1, save_path = path1+'/samplings/', select_IMPORTS = None):\n",
    "\n",
    "#     names = tuple(index)\n",
    "#     sampling_name = name\n",
    "\n",
    "#     l = []\n",
    "\n",
    "#     for y in years:\n",
    "#         print 'year: '+str(y)\n",
    "#         #Rows for each chunk\n",
    "#         size = np.ceil(nrows1[y]/n_chunks)\n",
    "#         print 'max_chunk_size: '+str(size)\n",
    "\n",
    "#         reader = pd.read_csv(data_path+'/DP1610_MAASTRICHT1_'+str(y)+'.txt', chunksize = size, \n",
    "#                              usecols = map(colname_no.get, columns)\n",
    "#                              , delimiter = ';', header = None) #'CN ID 8', 'PYOD'\n",
    "\n",
    "#         yr_result_list = []\n",
    "\n",
    "#         for i in range(int(n_chunks)):\n",
    "#             start_time = time.time()\n",
    "#             chunk = next(reader).rename(colno_name, axis = 1)\n",
    "#             chunk['IMPORT'] = chunk['FLUX'] % 2 # so that '1' indicates imports\n",
    "# #             print chunk.head()\n",
    "\n",
    "#             if firm_filtering == True:\n",
    "#                 for i in range(len(firm_filtering_cols)):\n",
    "#                     chunk = chunk.loc[chunk[firm_filtering_cols[i]].isin(firm_filtering_arrays[i])]\n",
    "#                     print 'filtered_chunk_size: '+str(len(chunk))\n",
    "# #             print chunk.head()\n",
    "                    \n",
    "#             if time_filtering == True:\n",
    "#                 time_filtering_cols = ['MONTH']\n",
    "#                 for i in range(len(time_filtering_cols)):\n",
    "#                     chunk = chunk.loc[chunk[time_filtering_cols[i]].isin(time_filtering_arrays[i])]\n",
    "#                     print 'filtered_chunk_size: '+str(len(chunk))\n",
    "\n",
    "#             if select_IMPORTS != None: chunk = chunk.loc[chunk.IMPORT == select_IMPORTS]\n",
    "\n",
    "#             if chunk_agg: \n",
    "#                 result_chunk = aggregation(chunk, index, chunk_agg_func)\n",
    "#             else: \n",
    "#                 result_chunk = chunk\n",
    "                \n",
    "# #             print result_chunk.head()\n",
    "\n",
    "#             yr_result_list += [result_chunk]\n",
    "#             if verbose: print(\"Number of rows \",result_chunk.shape[0])\n",
    "#             if verbose: print(\"Loop \",i,\"took %s seconds\" % (time.time() - start_time))\n",
    "#             del(result_chunk) \n",
    "\n",
    "\n",
    "#         concat_result = pd.concat(yr_result_list)\n",
    "# #         print concat_result.head()\n",
    "        \n",
    "#         # Unique users vs Number of rows after the first computation    \n",
    "#         if verbose: print(\"size of concat_result:\", len(concat_result))\n",
    "# #         if verbose: print(\"unique firms in concat_result:\", len(concat_result.index.unique()))\n",
    "\n",
    "#         result = concat_result\n",
    "#         result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "\n",
    "#         if yr_chunks_agg:\n",
    "# #             yr_agg_result = result.groupby(index).agg(yr_agg_func)\n",
    "#             yr_agg_result = aggregation(result, index, yr_agg_func)\n",
    "#             yr_agg_result.index = pd.MultiIndex.from_tuples(list(yr_agg_result.index), names=names)\n",
    "#             yr_agg_result.columns = yr_agg_result.columns.droplevel(1)\n",
    "\n",
    "#         else:\n",
    "#             yr_agg_result = result\n",
    "\n",
    "#         if verbose: print(\"size of yr_agg_result:\", len(yr_agg_result))\n",
    "\n",
    "#         l += [yr_agg_result]\n",
    "\n",
    "#         if save_yr_agg_result:\n",
    "#             yr_agg_result.to_csv(save_path+'/'+sampling_name+'_'+str(y)+'.csv')\n",
    "    \n",
    "#     filename = save_path+'/'+sampling_name+'.csv'\n",
    "#     pd.concat(l).to_csv(filename, index = True)\n",
    "#     print 'saved at: '+str(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sizes_index = pd.read_csv('./../export_france/data/formatted/sizes_index.csv')\n",
    "\n",
    "# n = 10\n",
    "\n",
    "# sizes_index['log_exp_mma'] = np.log10(sizes_index.exp_mma).round(3)\n",
    "# cuts = pd.cut(sizes_index.groupby(sizes_index['log_exp_mma']).sum().cumsum()['exp_mma'],n, labels=range(n))\n",
    "# exp_index = sizes_index.merge(cuts.reset_index().rename({'exp_mma': 'exp_mma_cat'}, axis = 1), on = 'log_exp_mma').dropna(subset = ['exp_mma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Oct 4\n",
    "# sizes_index = pd.read_csv('./../export_france/data/formatted/sizes_index.csv', index_col=0)\n",
    "# n = 20\n",
    "# exp_index = sizes_index.dropna(subset = ['exp_mma'])\n",
    "# exp_index['exp_qcut_'+str(n)+'_label'] = pd.qcut(exp_index['exp_mma'], n, labels=range(n))\n",
    "# exp_index['exp_qcut_'+str(n)] = pd.qcut(exp_index['exp_mma'], n)\n",
    "\n",
    "# # imp_index = sizes_index.dropna(subset = ['imp_mma'])\n",
    "# # imp_index['imp_cut_'+str(n)+'_label'] = pd.cut(imp_index['imp_mma'], n, labels=range(n))\n",
    "# # imp_index['imp_cut_'+str(n)] = pd.cut(imp_index['imp_mma'], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Mns = range(1,13)\n",
    "# time_filtering_arrays = [Mns]\n",
    "\n",
    "# # use all firms\n",
    "# sample_n_firms = 'max'\n",
    "\n",
    "# years = list(reversed(range(1997, 2014)))\n",
    "\n",
    "# for bin_label in list(reversed(range(n))):\n",
    "#     print 'bin label: '+str(bin_label)\n",
    "#     IDs = exp_index.loc[exp_index.exp_mma_cat == bin_label].index\n",
    "#     print IDs\n",
    "# #     IDs = imp_index.loc[imp_index.imp_cut_20_label == bin_label].index\n",
    "#     columns = ['YEAR', 'MONTH', 'FLUX', 'ID', 'CN ID 8', 'VAT','VART']\n",
    "#     sample_data(columns, index = ['YEAR', 'MONTH', 'IMPORT', 'ID', 'CN ID 8', 'VAT'], chunk_agg_func = {'VART':['sum']}, \n",
    "#                 firm_filtering = True, firm_filtering_cols = ['ID'], firm_filtering_arrays = [IDs],\n",
    "#                 yr_agg_func = {'VART_sum':['sum']}, n_chunks = 10., \n",
    "#                 name = 'firm_sample_YMxpb_n'+str(n).zfill(2)+str(bin_label).zfill(2),\n",
    "#                 verbose = False, yr_chunks_agg = True, select_IMPORTS = 0, save_yr_agg_result = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
