{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract from database\n",
    "\n",
    "This notebook contains all queries to the source database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from numpy import log10, arange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Configure Dask settings\n",
    "dask.config.set({\n",
    "    'array.chunk-size': '50MB',  # Set default chunk size for Dask arrays/dataframes\n",
    "    'distributed.worker.memory.target': 0.6,  # Target fraction of memory to stay below\n",
    "    'distributed.worker.memory.spill': 0.7,  # Fraction at which we start spilling to disk\n",
    "    'distributed.worker.memory.pause': 0.8,  # Fraction at which we pause worker threads\n",
    "    'distributed.worker.memory.terminate': 0.95,  # Fraction at which we terminate the worker\n",
    "    'distributed.dashboard.link': '{scheme}://{host}:{port}/status',  # Enable Dask dashboard\n",
    "})\n",
    "\n",
    "# Create a local Dask cluster\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,  # Number of workers (logical cores)\n",
    "    threads_per_worker=1,  # Threads per worker (set to 1 for dedicated core per worker)\n",
    "    memory_limit='3.75GB'  # Memory limit per worker (total RAM / number of workers)\n",
    ")\n",
    "\n",
    "# Connect to the cluster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Now you can proceed with your Dask tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drive_path = '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/'\n",
    "save_path = './../../data/processed/'\n",
    "\n",
    "colnames = [u'YEAR', u'MONTH', u'FLUX', u'ID', u'DEPT', u'CN ID 8', u'CPA6',\n",
    "       u'PYOD', u'PAYP', u'VAT', u'PRIFAC', u'DEVFAC', u'VFTE', u'VART', u'D_MASSE', u'MASSE', u'USUP', u'USUP_MT']\n",
    "colname_no = dict(zip(colnames, range(18)))\n",
    "\n",
    "# # Function to read data for specified columns and years\n",
    "# def get_data(columns, drive_path, start_year=1997, end_year=2014):\n",
    "#     df_list = []\n",
    "#     for y in range(start_year, end_year):\n",
    "#         df = dd.read_table(f'{drive_path}DP1610_MAASTRICHT1_{y}.txt', \n",
    "#                            usecols=list(map(colname_no.get, columns)),\n",
    "#                            delimiter=';', header=None, dtype={4: 'object', 6: 'object', 8: 'object', 9: 'object'}, \n",
    "#                            blocksize='50MB')\n",
    "#         df.columns = columns  # Assigning column names\n",
    "#         df_list.append(df)\n",
    "#     return dd.concat(df_list)\n",
    "\n",
    "\n",
    "def get_data(columns, drive_path, start_year=1997, end_year=2014):\n",
    "    df_list = []\n",
    "    for y in range(start_year, end_year):\n",
    "        df = dd.read_table(\n",
    "            f'{drive_path}DP1610_MAASTRICHT1_{y}.txt',\n",
    "            usecols=list(map(colname_no.get, columns)),\n",
    "            delimiter=';',\n",
    "            header=None,\n",
    "            names=columns,  # Set column names directly\n",
    "            dtype={4: 'object', 6: 'object', 8: 'object', 9: 'object'}, \n",
    "            blocksize='50MB'\n",
    "        )\n",
    "        df_list.append(df)\n",
    "    return dd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eperiment for block size\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "# import time\n",
    "\n",
    "# def perform_computation(blocksize):\n",
    "#     ddf = dd.read_table(f'{drive_path}DP1610_MAASTRICHT1_{y}.txt', \n",
    "#                         usecols=list(map(colname_no.get, columns)),\n",
    "#                         delimiter=';', header=None, dtype={4: 'object', 6: 'object', 8: 'object', 9: 'object'}, \n",
    "#                         blocksize=blocksize)\n",
    "#     ddf.columns = columns\n",
    "#     start_time = time.time()\n",
    "#     result = ddf.groupby('YEAR')['VART'].mean().compute()\n",
    "#     return time.time() - start_time\n",
    "\n",
    "# # Experiment with different blocksizes\n",
    "# blocksizes = ['10MB', '20MB', '30MB']\n",
    "# results = {}\n",
    "\n",
    "# for blocksize in blocksizes:\n",
    "#     print(f\"Testing blocksize: {blocksize}\")\n",
    "#     execution_time = perform_computation(blocksize)\n",
    "#     print(f\"Blocksize: {blocksize}, Execution Time: {execution_time:.2f} seconds\")\n",
    "#     results[blocksize] = execution_time\n",
    "\n",
    "# # Analyzing results\n",
    "# print(\"Experiment Results:\")\n",
    "# for blocksize, execution_time in results.items():\n",
    "#     print(f\"Blocksize: {blocksize}, Execution Time: {execution_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# # Experiment Results:\n",
    "# # Blocksize: 10MB, Execution Time: 86.68 seconds\n",
    "# # Blocksize: 20MB, Execution Time: 60.58 seconds\n",
    "# # Blocksize: 30MB, Execution Time: 62.73 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def chunk(s):\n",
    "    '''\n",
    "    The function applied to the\n",
    "    individual partition (map)\n",
    "    '''    \n",
    "    return s.apply(lambda x: list(set(x)))\n",
    "\n",
    "\n",
    "def agg(s):\n",
    "    '''\n",
    "    The function whic will aggrgate \n",
    "    the result from all the partitions(reduce)\n",
    "    '''\n",
    "    s = s._selected_obj    \n",
    "    return s.groupby(level=list(range(s.index.nlevels))).sum()\n",
    "\n",
    "\n",
    "def finalize(s):\n",
    "    '''\n",
    "    The optional functional that will be \n",
    "    applied to the result of the agg_tu functions\n",
    "    '''\n",
    "    return s.apply(lambda x: len(set(x)))\n",
    "\n",
    "tunique = dd.Aggregation('tunique', chunk, agg,finalize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets\n",
    "### - Price and quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# columns = [u'YEAR', u'MONTH', u'FLUX', u'ID', u'CN ID 8', u'PYOD', u'VART', u'MASSE', u'USUP', u'USUP_MT']\n",
    "\n",
    "\n",
    "# # Function to process price and quantities dataset\n",
    "# def process_price_quantities(columns, drive_path, end_year):\n",
    "#     data = get_data(columns, drive_path, end_year=end_year)\n",
    "#     # display(data.head())\n",
    "#     grouped = data[data.FLUX == 2].groupby(['ID', 'CN ID 8', 'MONTH', 'YEAR'])\n",
    "#     yearly_qv = grouped[['VART', 'MASSE']].sum().compute()\n",
    "#     yearly_qv.to_csv(f'{save_path}units_qv.csv')\n",
    "#     return yearly_qv\n",
    "\n",
    "# yearly_qv = process_price_quantities(columns, drive_path, end_year=2014)\n",
    "# print(yearly_qv.head())\n",
    "\n",
    "# # with ProgressBar():\n",
    "# #     yearly_details = data_.loc[data_.FLUX == 2].head(1000).groupby(['ID', 'CN ID 8', 'YEAR']).agg(\n",
    "# #         {'VART': sum, 'MASSE': sum, 'USUP': tunique, 'USUP': first, 'USUP_MT': sum}).compute()\n",
    "# # yearly_details.to_csv(save_path + 'units_detail.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Firm sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [u'YEAR', u'MONTH', u'FLUX', u'ID', u'VAT', u'VART']\n",
    "data = get_data(columns, drive_path, end_year = 1999)\n",
    "\n",
    "data['IMPORT'] = data['FLUX'] % 2\n",
    "\n",
    "firm_sizes = data.groupby(['ID', 'IMPORT','YEAR'])[['VART']].sum().reset_index()\n",
    "buyr_sizes = data.groupby(['VAT', 'IMPORT','YEAR'])[['VART']].sum().reset_index()\n",
    "\n",
    "with ProgressBar():\n",
    "    firm_sizes = firm_sizes.compute()\n",
    "    buyr_sizes = buyr_sizes.compute()\n",
    "    \n",
    "\n",
    "firm_sizes.to_csv(save_path + 'firm_sizes_99.csv', index = False)\n",
    "buyr_sizes.to_csv(save_path + 'buyr_sizes_99.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Value of buyer-seller links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 122.56 s\n"
     ]
    }
   ],
   "source": [
    "columns = [u'YEAR', u'FLUX', u'ID', u'VAT', u'VART']\n",
    "\n",
    "data = get_data(columns, drive_path, end_year = 1999)\n",
    "\n",
    "data['IMPORT'] = data['FLUX'] % 2\n",
    "\n",
    "links = data.groupby(['IMPORT','YEAR','ID','VAT'])['VART'].sum().reset_index()\n",
    "\n",
    "with ProgressBar():\n",
    "    out = links.compute()\n",
    "out.to_csv(save_path + 'buyer_seller_link_value.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMPORT</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ID</th>\n",
       "      <th>VAT</th>\n",
       "      <th>VART</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>215</td>\n",
       "      <td>IT0018705</td>\n",
       "      <td>569748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>330</td>\n",
       "      <td>IT0224285</td>\n",
       "      <td>14459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>413</td>\n",
       "      <td>AT0026337</td>\n",
       "      <td>69955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>413</td>\n",
       "      <td>BE1043987</td>\n",
       "      <td>41595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>413</td>\n",
       "      <td>DE0161168</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IMPORT  YEAR   ID        VAT    VART\n",
       "0       0  1997  215  IT0018705  569748\n",
       "1       0  1997  330  IT0224285   14459\n",
       "2       0  1997  413  AT0026337   69955\n",
       "3       0  1997  413  BE1043987   41595\n",
       "4       0  1997  413  DE0161168     294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(save_path + 'buyer_seller_link_value.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Sourcing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 187.61 s\n",
      "[########################################] | 100% Completed | 340.92 s\n"
     ]
    }
   ],
   "source": [
    "# columns = [u'YEAR', u'MONTH', u'FLUX', u'ID', u'CN ID 8', 'PYOD', u'VART']\n",
    "\n",
    "# data = get_data(columns, drive_path, end_year = 1999)\n",
    "\n",
    "# data['IMPORT'] = data['FLUX'] % 2\n",
    "# data['QUARTER'] = ((data['MONTH'] -1)// 3) + 1\n",
    "\n",
    "# CN_full = pd.read_csv('./../../data/processed/CN_full.csv', encoding = 'utf-8')\n",
    "# data = data.merge(CN_full[['CN ID 8', 'CN ID 4', 'CN label 4']])#.persist()\n",
    "\n",
    "# # Compute and save\n",
    "# sourcing_strategies = data.loc[data.IMPORT == 1].groupby(['YEAR', 'ID', 'CN ID 4', 'PYOD'])[['VART']].sum() #rm QUARTER for yearly dataset\n",
    "# with ProgressBar():\n",
    "#     out = sourcing_strategies.compute()\n",
    "# out.to_csv(save_path + 'sourcing_strategies_99.csv')\n",
    "\n",
    "# export_bundles = data.loc[data.IMPORT == 0].groupby(['YEAR', 'ID', 'CN ID 4', 'PYOD'])[['VART']].sum()\n",
    "# with ProgressBar():\n",
    "#     out2 = export_bundles.compute()\n",
    "# out2.to_csv(save_path + 'export_bundles_99.csv')\n",
    "\n",
    "# # Compute and save\n",
    "# sourcing_strategies_qr = data.loc[data.IMPORT == 1].groupby(['YEAR', 'QUARTER','ID', 'CN ID 4', 'PYOD'])[['VART']].sum() #rm QUARTER for yearly dataset\n",
    "# with ProgressBar():\n",
    "#     out = sourcing_strategies_qr.compute()\n",
    "# out.to_csv(save_path + 'sourcing_strategies_99_qr.csv')\n",
    "\n",
    "# export_bundles_qr = data.loc[data.IMPORT == 0].groupby(['YEAR', 'QUARTER', 'ID', 'CN ID 4', 'PYOD'])[['VART']].sum()\n",
    "# with ProgressBar():\n",
    "#     out2 = export_bundles_qr.compute()\n",
    "# out2.to_csv(save_path + 'export_bundles_99_qr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def compute_and_save_grouped_data(data, group_cols, agg_col, save_path, file_name, extra_group_cols=None):\n",
    "    if extra_group_cols is not None:\n",
    "        group_cols.extend(extra_group_cols)\n",
    "\n",
    "    grouped_data = data.groupby(group_cols)[[agg_col]].sum()\n",
    "    with ProgressBar():\n",
    "        output = grouped_data.compute()\n",
    "    output.to_csv(f'{save_path}{file_name}.csv')\n",
    "    return output\n",
    "\n",
    "def add_import_quarter_columns(data):\n",
    "    data['IMPORT'] = data['FLUX'] % 2\n",
    "    data['QUARTER'] = ((data['MONTH'] - 1) // 3) + 1\n",
    "    return data\n",
    "\n",
    "# Main processing function\n",
    "def process_sourcing_export(data, drive_path, end_year, save_path):\n",
    "    data = get_data(columns, drive_path, end_year=end_year)\n",
    "    data = add_import_quarter_columns(data)\n",
    "    \n",
    "    # Merging additional data\n",
    "    CN_full = pd.read_csv('./../../data/processed/CN_full.csv', encoding='utf-8')\n",
    "    data = data.merge(CN_full[['CN ID 8', 'CN ID 4', 'CN label 4']])\n",
    "    \n",
    "    # Compute and save\n",
    "    compute_and_save_grouped_data(data[data.IMPORT == 1], \n",
    "                                  ['YEAR', 'ID', 'CN ID 4', 'PYOD'], \n",
    "                                  'VART', save_path, 'sourcing_strategies_99')\n",
    "\n",
    "    compute_and_save_grouped_data(data[data.IMPORT == 0], \n",
    "                                  ['YEAR', 'ID', 'CN ID 4', 'PYOD'], \n",
    "                                  'VART', save_path, 'export_bundles_99')\n",
    "\n",
    "    # For quarterly data\n",
    "    compute_and_save_grouped_data(data[data.IMPORT == 1], \n",
    "                                  ['YEAR', 'ID', 'CN ID 4', 'PYOD'], \n",
    "                                  'VART', save_path, 'sourcing_strategies_99_qr', \n",
    "                                  extra_group_cols=['QUARTER'])\n",
    "\n",
    "    compute_and_save_grouped_data(data[data.IMPORT == 0], \n",
    "                                  ['YEAR', 'ID', 'CN ID 4', 'PYOD'], \n",
    "                                  'VART', save_path, 'export_bundles_99_qr', \n",
    "                                  extra_group_cols=['QUARTER'])\n",
    "\n",
    "# Usage\n",
    "columns = [u'YEAR', u'MONTH', u'FLUX', u'ID', u'CN ID 8', 'PYOD', u'VART']\n",
    "drive_path = # Define drive path\n",
    "end_year = 1999\n",
    "save_path = # Define save path\n",
    "\n",
    "process_sourcing_export(data, drive_path, end_year, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ID</th>\n",
       "      <th>CN ID 4</th>\n",
       "      <th>PYOD</th>\n",
       "      <th>VART</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>705</td>\n",
       "      <td>CH</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>709</td>\n",
       "      <td>CH</td>\n",
       "      <td>45022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>1806</td>\n",
       "      <td>JP</td>\n",
       "      <td>25972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>2204</td>\n",
       "      <td>BS</td>\n",
       "      <td>2389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>2204</td>\n",
       "      <td>CA</td>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  ID  CN ID 4 PYOD   VART\n",
       "0  1997   0      705   CH  71651\n",
       "1  1997   0      709   CH  45022\n",
       "2  1997   0     1806   JP  25972\n",
       "3  1997   0     2204   BS   2389\n",
       "4  1997   0     2204   CA   1512"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(save_path + 'export_bundles_99.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bernard's margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 120.90 ms"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##                                      ] | 5% Completed | 21.22 sms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database Data Extraction and Setup.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Compute the results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m ProgressBar():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     result \u001b[39m=\u001b[39m grouped\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Save the results to CSV\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m result\u001b[39m.\u001b[39mto_csv(save_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbernards_margins.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/base.py:381\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    358\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/base.py:666\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    664\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 666\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    667\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Define the columns needed\n",
    "columns = [u'YEAR', u'FLUX', u'ID', u'CN ID 8', 'PYOD', u'VART', u'VAT']\n",
    "\n",
    "# Load the data\n",
    "data = get_data(columns, drive_path, end_year=1999)\n",
    "\n",
    "# Apply transformation\n",
    "data['IMPORT'] = data['FLUX'] % 2\n",
    "\n",
    "# Group and aggregate the data\n",
    "grouped = data.groupby(['IMPORT', 'YEAR', 'ID']).agg({'VAT': tunique, 'PYOD': tunique, 'CN ID 8': tunique, 'VART': 'sum'})\n",
    "\n",
    "# Compute the results\n",
    "with ProgressBar():\n",
    "    result = grouped.compute()\n",
    "\n",
    "# Save the results to CSV\n",
    "result.to_csv(save_path + 'bernards_margins.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Krammar's determinants of diversification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 120.35 ms"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 15m 26s\n"
     ]
    }
   ],
   "source": [
    "columns = [u'YEAR', u'FLUX', u'ID', u'CN ID 8', u'VAT', u'PYOD', u'VART']\n",
    "\n",
    "data = get_data(columns, drive_path, end_year = 1999)\n",
    "data['IMPORT'] = data['FLUX'] % 2\n",
    "\n",
    "grouped = data.groupby(['ID', 'YEAR', 'IMPORT'])\n",
    "\n",
    "with ProgressBar():\n",
    "    df = grouped.agg({'VART': 'sum', u'VAT': tunique, 'CN ID 8': tunique, u'PYOD': tunique}).compute()\n",
    "\n",
    "df.to_csv(save_path + 'dets_of_diversification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.read_csv(save_path + 'dets_of_diversification.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ID degree for year 1997 and window 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database Data Extraction and Setup.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#Y100sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m ID_degree[\u001b[39m'\u001b[39m\u001b[39mcenter_year\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m Yc\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#Y100sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m ID_degree[\u001b[39m'\u001b[39m\u001b[39mwindow\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m window\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#Y100sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m ID_degree[\u001b[39m'\u001b[39m\u001b[39mbin\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mcut(log10(ID_degree[\u001b[39m'\u001b[39;49m\u001b[39mID_degree\u001b[39;49m\u001b[39m'\u001b[39;49m]), bins\u001b[39m=\u001b[39;49marange(\u001b[39m-\u001b[39;49m\u001b[39m.49\u001b[39;49m, \u001b[39m5.99\u001b[39;49m, \u001b[39m.25\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#Y100sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m filename_ID \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msave_path\u001b[39m}\u001b[39;00m\u001b[39mID_deg_\u001b[39m\u001b[39m{\u001b[39;00mYc\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mwindow\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#Y100sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mwith\u001b[39;00m ProgressBar():\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/core/reshape/tile.py:293\u001b[0m, in \u001b[0;36mcut\u001b[0;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m (np\u001b[39m.\u001b[39mdiff(bins\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat64\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many():\n\u001b[1;32m    291\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbins must increase monotonically.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 293\u001b[0m fac, bins \u001b[39m=\u001b[39m _bins_to_cuts(\n\u001b[1;32m    294\u001b[0m     x,\n\u001b[1;32m    295\u001b[0m     bins,\n\u001b[1;32m    296\u001b[0m     right\u001b[39m=\u001b[39;49mright,\n\u001b[1;32m    297\u001b[0m     labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    298\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    299\u001b[0m     include_lowest\u001b[39m=\u001b[39;49minclude_lowest,\n\u001b[1;32m    300\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    301\u001b[0m     duplicates\u001b[39m=\u001b[39;49mduplicates,\n\u001b[1;32m    302\u001b[0m     ordered\u001b[39m=\u001b[39;49mordered,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    305\u001b[0m \u001b[39mreturn\u001b[39;00m _postprocess_for_cut(fac, bins, retbins, dtype, original)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/core/reshape/tile.py:427\u001b[0m, in \u001b[0;36m_bins_to_cuts\u001b[0;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered)\u001b[0m\n\u001b[1;32m    424\u001b[0m     bins \u001b[39m=\u001b[39m unique_bins\n\u001b[1;32m    426\u001b[0m side: Literal[\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m right \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 427\u001b[0m ids \u001b[39m=\u001b[39m ensure_platform_int(bins\u001b[39m.\u001b[39;49msearchsorted(x, side\u001b[39m=\u001b[39;49mside))\n\u001b[1;32m    429\u001b[0m \u001b[39mif\u001b[39;00m include_lowest:\n\u001b[1;32m    430\u001b[0m     ids[np\u001b[39m.\u001b[39masarray(x) \u001b[39m==\u001b[39m bins[\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/dataframe/core.py:602\u001b[0m, in \u001b[0;36m_Frame.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m    603\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed)\n\u001b[1;32m    604\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/base.py:381\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    358\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/base.py:666\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    664\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 666\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    667\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import arange, log10\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Settings\n",
    "center_years = arange(1997, 2000, 2)\n",
    "window = 1\n",
    "gap = (window - 1) / 2\n",
    "save_path = './../../data/processed/'   # Define where to save the output file\n",
    "\n",
    "\n",
    "# Load data (you need to define how 'data' is loaded here)\n",
    "# data = ...\n",
    "\n",
    "ID_degree_res = []\n",
    "VAT_degree_res = []\n",
    "\n",
    "# Process ID degrees\n",
    "for Yc in center_years:\n",
    "    print(f\"Processing ID degree for year {Yc} and window {window}\")\n",
    "    data_sec = data.loc[data.YEAR - Yc <= gap]\n",
    "    \n",
    "    grouped_ID = data_sec.groupby(['ID']).agg({'VAT': tunique, 'VART': sum})\n",
    "    ID_degree = grouped_ID[['VAT']].reset_index()\n",
    "    ID_degree.columns = ['ID', 'ID_degree']\n",
    "    ID_degree['center_year'] = Yc\n",
    "    ID_degree['window'] = window\n",
    "    ID_degree['bin'] = pd.cut(log10(ID_degree['ID_degree']), bins=arange(-.49, 5.99, .25))\n",
    "    \n",
    "    filename_ID = f'{save_path}ID_deg_{Yc}_{window}.csv'\n",
    "    with ProgressBar():\n",
    "        ID_degree.compute().to_csv(filename_ID, index=False)\n",
    "    print(f\"Saved to {filename_ID}\")\n",
    "    ID_degree_res.append(ID_degree)\n",
    "\n",
    "# Process VAT degrees\n",
    "for Yc in center_years:\n",
    "    print(f\"Processing VAT degree for year {Yc} and window {window}\")\n",
    "    ID_deg = ID_degree_res[center_years.index(Yc)]\n",
    "    \n",
    "    sampling = ID_deg.groupby(['bin'], observed=True).apply(lambda x: x.sample(200, replace=True))\n",
    "    data_sec_sample = data.loc[data.ID.isin(sampling['ID'].values)]\n",
    "    \n",
    "    grouped_VAT = data_sec_sample.groupby(['VAT']).agg({'ID': tunique, 'VART': sum})\n",
    "    VAT_degree = grouped_VAT[['ID']].reset_index()\n",
    "    VAT_degree.columns = ['VAT', 'VAT_degree']\n",
    "    VAT_degree['center_year'] = Yc\n",
    "    VAT_degree['window'] = window\n",
    "    \n",
    "    filename_VAT = f'{save_path}VAT_deg_{Yc}_{window}.csv'\n",
    "    with ProgressBar():\n",
    "        VAT_degree.compute().to_csv(filename_VAT, index=False)\n",
    "    print(f\"Saved to {filename_VAT}\")\n",
    "    VAT_degree_res.append(VAT_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1997\n",
      "[########################################] | 100% Completed |  7min  3.7s\n",
      "[########################################] | 100% Completed |  4min 30.6s\n",
      "1999\n",
      "[########################################] | 100% Completed | 14min  4.1s\n",
      "[########################################] | 100% Completed |  8min  6.8s\n"
     ]
    }
   ],
   "source": [
    "# # window = 3\n",
    "# # assortativity_res = []\n",
    "# ID_degree_res = []\n",
    "# VAT_degree_res = []\n",
    "\n",
    "# for window in [1]:\n",
    "#     gap = (window-1)/2\n",
    "#     center_years = arange(1997, 2000, 2)\n",
    "#     print window\n",
    "\n",
    "#     for Yc in center_years:\n",
    "#         print Yc\n",
    "#         data_sec = data.loc[data.YEAR - Yc <= gap]\n",
    "# #         data_sec.groupby(['ID', 'VAT']).agg({'VART': sum })\n",
    "\n",
    "#         data_sec_by_ID = data_sec.groupby(['ID']).agg({'VAT': tunique, 'VART': sum})\n",
    "\n",
    "#         ID_degree = data_sec_by_ID[['VAT']].reset_index()\n",
    "#         ID_degree.columns = [u'ID', u'ID_degree']\n",
    "#         ID_degree['center_year'] = Yc\n",
    "#         ID_degree['window'] = window\n",
    "        \n",
    "#         with ProgressBar():\n",
    "#             ID_deg = ID_degree.compute()\n",
    "#             ID_deg['bin'] = pd.cut(log10(ID_deg['ID_degree']), bins = arange(-.49, 5.99, .25))\n",
    "#             ID_deg.to_csv(save_path + 'ID_deg_'+str(Yc)+'_'+str(window)+'.csv', index = False)\n",
    "# #         ID_degree_res += [ID_degree]     \n",
    "\n",
    "# #         ID_deg = pd.read_csv()\n",
    "#         sampling = ID_deg.groupby(['bin'], observed = True).apply(lambda x: x.sample(200, replace = True))\n",
    "\n",
    "#         data_sec_sample = data_sec.loc[data_sec.ID.isin(sampling['ID'].values)]\n",
    "#         data_sec_by_VAT = data_sec_sample.groupby(['VAT']).agg({'ID': tunique, 'VART': sum})\n",
    "\n",
    "#         VAT_degree = data_sec_by_VAT[['ID']].reset_index()\n",
    "#         VAT_degree.columns = [u'VAT', u'VAT_degree']\n",
    "#         VAT_degree['center_year'] = Yc\n",
    "#         VAT_degree['window'] = window\n",
    "#         VAT_degree_res += [VAT_degree]\n",
    "#         with ProgressBar():\n",
    "#             VAT_deg = VAT_degree.compute()\n",
    "#             VAT_deg.to_csv(save_path + 'VAT_deg_'+str(Yc)+'_'+str(window)+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.read_csv(save_path + 'ID_deg_'+str(Yc)+'_'+str(window)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_degrees' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-23802a306fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_degrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VAT_degree_bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID_degree'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'VAT_degree'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'VAT_degree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ID_degree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_degrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VAT_degree_bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID_degree'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'VAT_degree'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'VAT_degree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ID_degree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_degrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VAT_degree_bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID_degree'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'VAT_degree'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'VAT_degree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ID_degree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_degrees' is not defined"
     ]
    }
   ],
   "source": [
    "# fig, ax = plt.subplots(1)\n",
    "# df_degrees.groupby('VAT_degree_bin')['ID_degree','VAT_degree'].quantile(.25).plot(x = 'VAT_degree', y = 'ID_degree', marker = '', ax = ax)\n",
    "# df_degrees.groupby('VAT_degree_bin')['ID_degree','VAT_degree'].quantile(.5).plot(x = 'VAT_degree', y = 'ID_degree', marker = '', ax = ax)\n",
    "# df_degrees.groupby('VAT_degree_bin')['ID_degree','VAT_degree'].quantile(.75).plot(x = 'VAT_degree', y = 'ID_degree', marker = '', ax = ax)\n",
    "\n",
    "# # df_degrees.groupby('ID_nunique_bin')['VAT_nunique','ID_nunique'].mean().plot(x = 'ID_nunique', y = 'VAT_nunique', marker = 'o', ax = ax)\n",
    "# df_degrees.groupby('ID_nunique')['VAT_nunique'].median().plot(x = 'index', y = 'VAT_nunique', marker = '.', linewidth = 0, ax = ax)\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Buyers and sellers by foreign country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 3.16 s ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database Data Extraction and Setup.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m result \u001b[39m=\u001b[39m data_by_dest\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mPYOD\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mYEAR\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m: tunique, \u001b[39m'\u001b[39m\u001b[39mVART\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m ProgressBar():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     out \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Data_Extraction_and_Preparation/Database%20Data%20Extraction%20and%20Setup.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m out\u001b[39m.\u001b[39mto_csv(save_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdestination.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/base.py:381\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    358\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/base.py:666\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    664\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 666\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    667\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = './../../data/processed/'   # Define where to save the output file\n",
    "columns = [u'YEAR', u'FLUX', u'ID', u'PYOD', u'VART']\n",
    "data = get_data(columns, drive_path, end_year = 1999)\n",
    "data['IMPORT'] = data['FLUX'] % 2\n",
    "\n",
    "data_by_dest = data.groupby(['IMPORT','YEAR','ID','PYOD'])['VART'].sum().reset_index()\n",
    "\n",
    "result = data_by_dest.groupby(['PYOD', 'YEAR']).agg({'ID': tunique, 'VART': 'sum'})\n",
    "\n",
    "with ProgressBar():\n",
    "    out = result.compute()\n",
    "    \n",
    "out.to_csv(save_path + 'destination.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### - Size distribution of firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 103.46 s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Settings\n",
    "drive_path = '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/'  # Define the path to your data\n",
    "end_year = 1999    # Define the end year for your data\n",
    "save_path = './../../data/processed/'   # Define where to save the output file\n",
    "n_bins = 20        # Number of bins for categorizing data\n",
    "\n",
    "# Load and process the data\n",
    "columns = ['YEAR', 'FLUX', 'ID', 'VART']\n",
    "data = get_data(columns, drive_path, end_year=end_year)\n",
    "data['IMPORT'] = data['FLUX'] % 2\n",
    "\n",
    "# Group by IMPORT, YEAR, ID and sum the VART column\n",
    "grouped_data = data.groupby(['IMPORT', 'YEAR', 'ID'])['VART'].sum().reset_index()\n",
    "\n",
    "# Compute and convert to Pandas dataframe\n",
    "with ProgressBar():\n",
    "    filtered_data = grouped_data[grouped_data['VART'] > 0].compute()\n",
    "\n",
    "\n",
    "# Calculate the median of VART for each ID and IMPORT\n",
    "median_sizes = filtered_data.groupby(['ID', 'IMPORT'])['VART'].median().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "median_sizes = median_sizes.rename(columns={'VART': 'exp_mma'})\n",
    "\n",
    "# Log transform and binning\n",
    "median_sizes['log_exp_mma'] = np.log10(median_sizes['exp_mma']).round(3)\n",
    "cuts = pd.cut(median_sizes['log_exp_mma'], n_bins, labels=range(n_bins))\n",
    "median_sizes['exp_mma_cat'] = cuts\n",
    "\n",
    "# Save the processed data\n",
    "median_sizes.to_csv(f'{save_path}sizes_index.csv', index=False)\n",
    "\n",
    "# Now 'median_sizes' can be used similar to 'sizes_index' in your previous script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>IMPORT</th>\n",
       "      <th>exp_mma</th>\n",
       "      <th>log_exp_mma</th>\n",
       "      <th>exp_mma_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32949420.0</td>\n",
       "      <td>7.518</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38070333.0</td>\n",
       "      <td>7.581</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>3.311</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>77846.0</td>\n",
       "      <td>4.891</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>732059.5</td>\n",
       "      <td>5.865</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  IMPORT     exp_mma  log_exp_mma exp_mma_cat\n",
       "0    0       0  32949420.0        7.518          14\n",
       "1    0       1  38070333.0        7.581          14\n",
       "2   18       1      2047.0        3.311           6\n",
       "3  157       1     77846.0        4.891           9\n",
       "4  215       0    732059.5        5.865          11"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_sizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import log10, arange\n",
    "\n",
    "# Load the data and create time periods\n",
    "links = pd.read_csv(save_path + 'buyer_seller_link_value.csv')\n",
    "links['PERIOD'] = (links['YEAR'] - 1996) // 2\n",
    "\n",
    "# Calculate degrees\n",
    "degrees = links.groupby(['PERIOD', 'ID'])[['VAT']].nunique().rename(columns = {'VAT': 'ID_degree'})\n",
    "\n",
    "# Log transform and binning\n",
    "degrees['log_ID_degree'] = log10(degrees['ID_degree'])\n",
    "degrees['bin_ID_degree'] = pd.cut(degrees['log_ID_degree'], arange(-.25, 4.5, 0.25))\n",
    "\n",
    "# Calculate degree distribution\n",
    "degree_dist = degrees.reset_index().groupby(['PERIOD', 'bin_ID_degree'])[['ID']].count()\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(1, 2, figsize =(15, 6))\n",
    "ax = axs[0]\n",
    "for t in links['PERIOD'].unique():\n",
    "    log10(degree_dist.loc[t]).reset_index().plot(marker='o', linewidth=0, ax=ax, mec='None')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
