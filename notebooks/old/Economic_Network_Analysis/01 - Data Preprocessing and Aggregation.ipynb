{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Notebook settings for better control over execution and display\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data paths for reading and saving files\n",
    "drive_path = '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/'\n",
    "save_path = './../../data/processed/'\n",
    "\n",
    "# Column names and their corresponding numbers in the raw data files\n",
    "colnames = ['YEAR', 'MONTH', 'FLUX', 'ID', 'DEPT', 'CN ID 8', 'CPA6', 'PYOD', 'PAYP', 'VAT', \n",
    "            'PRIFAC', 'DEVFAC', 'VFTE', 'VART', 'D_MASSE', 'MASSE', 'USUP', 'USUP_MT']\n",
    "colname_no = {name: idx for idx, name in enumerate(colnames)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and preprocess the data\n",
    "def get_data(columns, start_year, end_year):\n",
    "    dtype = {4: 'object', 9: 'object'}  # Specify the correct dtype for known columns\n",
    "    df_list = []\n",
    "    for y in range(start_year, end_year):\n",
    "        df = dd.read_csv(f'{drive_path}DP1610_MAASTRICHT1_{y}.txt', usecols=columns, \n",
    "                         delimiter=';', header=None, dtype=dtype, blocksize='100MB')\n",
    "        df.columns = [colnames[i] for i in columns]  # Set the column names\n",
    "        df_list.append(df)\n",
    "    return dd.concat(df_list)\n",
    "\n",
    "# Example usage:\n",
    "# columns_to_load = [0, 2, 3, 5, 7, 13]  # YEAR, FLUX, ID, CN ID 8, PYOD, VART\n",
    "# data = get_data(columns_to_load, 1997, 2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and save firm sizes and buyer-seller links\n",
    "def compute_firm_sizes_and_links(data):\n",
    "    firm_sizes = data.groupby(['ID', 'IMPORT', 'YEAR'])['VART'].sum().reset_index()\n",
    "    buyer_seller_links = data.groupby(['ID', 'VAT', 'YEAR'])['VART'].sum().reset_index()\n",
    "\n",
    "    # Persisting the results to avoid re-computation\n",
    "    # firm_sizes.to_csv(f'{save_path}firm_sizes.csv', index=False)\n",
    "    # buyer_seller_links.to_csv(f'{save_path}buyer_seller_links.csv', index=False)\n",
    "    firm_sizes.to_csv(f'{save_path}firm_sizes.csv', index=False, single_file=True)\n",
    "    buyer_seller_links.to_csv(f'{save_path}buyer_seller_links.csv', index=False, single_file=True)\n",
    "\n",
    "\n",
    "    return firm_sizes, buyer_seller_links\n",
    "\n",
    "# Example usage:\n",
    "# firm_sizes, buyer_seller_links = compute_firm_sizes_and_links(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileNotFoundError: [Errno 2] No such file or directory: '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997.txt'\n",
    "# '/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01 - Data Preprocessing and Aggregation.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m get_data(columns_to_load, \u001b[39m1997\u001b[39m, \u001b[39m1999\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mIMPORT\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mFLUX\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m firm_sizes, buyer_seller_links \u001b[39m=\u001b[39m compute_firm_sizes_and_links(data)\n",
      "\u001b[1;32m/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01 - Data Preprocessing and Aggregation.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m buyer_seller_links \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVAT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mYEAR\u001b[39m\u001b[39m'\u001b[39m])[\u001b[39m'\u001b[39m\u001b[39mVART\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Persisting the results to avoid re-computation\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# firm_sizes.to_csv(f'{save_path}firm_sizes.csv', index=False)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# buyer_seller_links.to_csv(f'{save_path}buyer_seller_links.csv', index=False)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m firm_sizes\u001b[39m.\u001b[39;49mto_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00msave_path\u001b[39m}\u001b[39;49;00m\u001b[39mfirm_sizes.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, single_file\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m buyer_seller_links\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msave_path\u001b[39m}\u001b[39;00m\u001b[39mbuyer_seller_links.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, single_file\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matias/repos/French_exporters/notebooks/Economic_Network_Analysis/01%20-%20Data%20Preprocessing%20and%20Aggregation.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m firm_sizes, buyer_seller_links\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/dataframe/core.py:1897\u001b[0m, in \u001b[0;36m_Frame.to_csv\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See dd.to_csv docstring for more information\"\"\"\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m to_csv\n\u001b[0;32m-> 1897\u001b[0m \u001b[39mreturn\u001b[39;00m to_csv(\u001b[39mself\u001b[39;49m, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/dataframe/io/csv.py:995\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(df, filename, single_file, encoding, mode, name_function, compression, compute, scheduler, storage_options, header_first_partition_only, compute_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m         compute_kwargs[\u001b[39m\"\u001b[39m\u001b[39mscheduler\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m scheduler\n\u001b[1;32m    993\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\n\u001b[0;32m--> 995\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(dask\u001b[39m.\u001b[39;49mcompute(\u001b[39m*\u001b[39;49mvalues, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs))\n\u001b[1;32m    996\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    997\u001b[0m     \u001b[39mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/base.py:666\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    664\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 666\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    667\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "columns_to_load = [0, 2, 3, 5, 7, 9, 13]  # YEAR, FLUX, ID, CN ID 8, PYOD, VART\n",
    "data = get_data(columns_to_load, 1997, 1999)\n",
    "data['IMPORT'] = data['FLUX'] % 2\n",
    "\n",
    "firm_sizes, buyer_seller_links = compute_firm_sizes_and_links(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        DataFrame\n",
      "\u001b[0;31mString form:\u001b[0m\n",
      "Dask DataFrame Structure:\n",
      "           YEAR   FLUX     ID CN ID 8    PYOD     VAT   VART IMPO <...>         ...    ...    ...     ...     ...     ...    ...    ...\n",
      "           Dask Name: assign, 8 graph layers\n",
      "\u001b[0;31mLength:\u001b[0m      40209836\n",
      "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/envs/base2/lib/python3.11/site-packages/dask/dataframe/core.py\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "Parallel Pandas DataFrame\n",
      "\n",
      "Do not use this class directly.  Instead use functions like\n",
      "``dd.read_csv``, ``dd.read_parquet``, or ``dd.from_pandas``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "dsk: dict\n",
      "    The dask graph to compute this DataFrame\n",
      "name: str\n",
      "    The key prefix that specifies which keys in the dask comprise this\n",
      "    particular DataFrame\n",
      "meta: pandas.DataFrame\n",
      "    An empty ``pandas.DataFrame`` with names, dtypes, and index matching\n",
      "    the expected output.\n",
      "divisions: tuple of index values\n",
      "    Values along which we partition our blocks on the index"
     ]
    }
   ],
   "source": [
    "data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Delayed('int-fd56ec31-6f24-4381-979b-c3dc2ef978fd'), 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-14.0.2-cp311-cp311-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /home/matias/anaconda3/envs/base2/lib/python3.11/site-packages (from pyarrow) (1.24.3)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-14.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate results for later use\n",
    "firm_sizes.to_parquet(f'{save_path}firm_sizes.parquet')\n",
    "buyer_seller_links.to_parquet(f'{save_path}buyer_seller_links.parquet')\n",
    "\n",
    "# Save Dask dataframe as parquet for efficient access later on\n",
    "data.to_parquet(f'{save_path}full_dataset.parquet')\n",
    "# 7 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/DP1610_MAASTRICHT1_1998.txt'\n",
    "'/media/matias/Elements/export_france/data/type1/DP1610_MAASTRICHT1_1997_2013/DP1610_MAASTRICHT1_1998.txt'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
